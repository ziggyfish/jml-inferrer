\section{Related Work}
\label{sec:related}

This section surveys related work in specification inference, test generation, and formal methods for software maintenance.

\subsection{Formal Specification Inference}

\subsubsection{Weakest Precondition Analysis}

The weakest precondition calculus~\cite{dijkstra_guarded_1975, dijkstra1976discipline} and Hoare logic~\cite{hoare1969axiomatic} form the theoretical foundation of our approach. While these techniques have been extensively studied in verification contexts, their application to automated specification inference for practical software engineering is more recent.

Barnett and Leino~\cite{barnett2005weakest} developed the Boogie intermediate language, which uses WP reasoning for verifying .NET programs. Our work differs by focusing on specification \emph{inference} rather than verification against provided specifications.

\subsubsection{Dynamic Invariant Detection}

Daikon~\cite{ernst2007daikon} pioneered dynamic invariant detection, inferring likely specifications from program execution traces. Unlike our static approach:
\begin{itemize}
    \item Daikon requires execution traces, which may not cover all paths
    \item Inferred invariants are ``likely'' rather than guaranteed
    \item Applicability depends on test suite quality
\end{itemize}

Perkins et al.~\cite{perkins2009automatically} extended dynamic inference to detect and repair program errors. Our static approach complements dynamic methods by providing guaranteed-correct specifications that can seed dynamic refinement.

\subsubsection{Natural Language Processing Approaches}

Toradocu~\cite{toradocu2016} and its successor Jdoctor~\cite{jdoctor2018} use NLP to extract specifications from Javadoc comments. As discussed in Section~\ref{sec:results}, these tools are complementary to our approach:
\begin{itemize}
    \item Jdoctor achieves 92\% precision and 83\% recall on documented exceptions
    \item Our tool achieves 94.2\% precision and 89.3\% recall from code analysis
    \item 735 specifications were uniquely found by Jdoctor; 1,091 uniquely by our tool
\end{itemize}

Pandita et al.~\cite{pandita2012inferring} used NLP to infer resource specifications from API documentation. Their approach is limited to documented behaviors, while our code-based analysis can infer undocumented constraints.

\subsubsection{Symbolic Execution Approaches}

PreInfer~\cite{preinfer2017} infers preconditions using symbolic execution via Microsoft's Pex framework. Key differences from our approach:
\begin{itemize}
    \item PreInfer uses dynamic symbolic execution; we use static analysis
    \item PreInfer handles complex conditions including quantifiers; we focus on scalable inference of simpler conditions
    \item PreInfer targets C\#; we target Java
\end{itemize}

\subsection{LLM-Based Specification and Test Generation}

\subsubsection{LLM Specification Inference}

Recent work has explored using LLMs for specification inference. Ma et al.~\cite{ma2024specgen} propose SpecGen, which uses GPT-4 to generate specifications from code. Compared to our approach:
\begin{itemize}
    \item LLMs achieve higher recall (91.2\% vs 89.3\%) but lower precision (78.6\% vs 94.2\%)
    \item LLM outputs are non-deterministic (67.3\% consistency across runs)
    \item Static analysis is reproducible and explainable
\end{itemize}

\subsubsection{LLM-Based Test Generation}

LLMs have shown promise for automated test generation~\cite{chen2022codet, schafer2023adaptive}. Sch\"{a}fer et al.~\cite{schafer2023adaptive} use adaptive test generation with LLMs, achieving 70\% compilation rates. Our work demonstrates that providing formal specifications to LLMs improves both compilation rates (91.5\%) and test quality (mutation score).

Lemieux et al.~\cite{lemieux2023codamosa} combine coverage-guided fuzzing with LLM test generation. Their approach could potentially benefit from specification-guided generation, which we leave as future work.

\subsection{Test Oracle Generation}

\subsubsection{Specification-Based Oracles}

Specification-based testing~\cite{zhu_software_1997, ammann2016introduction} has long used formal specifications as test oracles. Our contribution is automating the specification inference step, reducing the manual effort required.

Peters and Parnas~\cite{peters1998using} demonstrated using formal documentation as test oracles. Our work extends this by generating specifications where documentation is absent.

\subsubsection{Metamorphic Testing}

Metamorphic testing~\cite{chen2018metamorphic} uses metamorphic relations as implicit oracles. While not directly comparable, inferred specifications can be viewed as explicit oracles that enable traditional assertion-based testing.

\subsection{Formal Methods in Practice}

\subsubsection{Industrial Formal Verification}

Large-scale formal verification projects~\cite{andronick_large-scale_2012, klein2014sel4} demonstrate both the value and cost of formal methods. Matichuk et al.~\cite{matichuk_cost_2015} report that specification development accounts for approximately 50\% of formal verification effort. Our automation aims to reduce this barrier.

\subsubsection{Lightweight Formal Methods}

Zimmermann and Wehrheim~\cite{zimmermann2019lightweight} advocate for lightweight specifications compatible with DevOps practices. Our automatically inferred specifications align with this philosophy: they are machine-checkable but generated without manual annotation effort.

\subsubsection{Contract-Based Design}

Design by Contract~\cite{meyer1992design} and the JML language~\cite{jml} promote specification as a design activity. Our tool supports this paradigm by providing initial specifications that developers can refine.

\subsection{Program Comprehension}

\subsubsection{Method Stereotypes}

Dragan et al.~\cite{dragan2006reverse} introduced method stereotypes for reverse engineering. Our categorization taxonomy extends their work with categories relevant to specification inference. Table~\ref{tab:stereotype-comparison} maps our categories to their stereotypes.

\begin{table}[h]
\centering
\caption{Mapping to Dragan et al. stereotypes}
\label{tab:stereotype-comparison}
\begin{tabular}{ll}
\toprule
\textbf{Our Category} & \textbf{Dragan Stereotype} \\
\midrule
Accessors \& Mutators & Get, Set \\
Factory \& Delegate & Factory, Collaborator \\
Control Flow & Command, Controller \\
Utility & Property, Predicate \\
State Modification & Command, Non-void-Command \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Code Summarization}

LLM-based code summarization~\cite{ahmed2022few} can complement specifications by providing natural language descriptions. Specifications provide formal, machine-checkable contracts while summaries provide human-readable overviews.

\subsection{Mutation Testing}

PiTest~\cite{coles2016pitest} is the standard mutation testing tool for Java, which we use for evaluation. Prior work~\cite{papadakis2019mutation} surveys mutation testing practices; our results contribute evidence that specification-guided tests achieve higher mutation scores than ad-hoc generation.

\subsection{Summary of Positioning}

Our work occupies a unique position in the landscape:

\begin{enumerate}
    \item \textbf{Compared to dynamic analysis} (Daikon): We provide guaranteed-correct specifications without requiring execution traces.

    \item \textbf{Compared to NLP approaches} (Jdoctor): We infer specifications from code, not documentation, achieving broader coverage.

    \item \textbf{Compared to LLM approaches}: We achieve higher precision and determinism through formal analysis.

    \item \textbf{Compared to verification tools} (Boogie): We focus on inference rather than verification, reducing manual effort.
\end{enumerate}

The key novelty is demonstrating that WP/SP-based static analysis, combined with practical heuristics for loop handling, can scale to large codebases while maintaining high precision.
