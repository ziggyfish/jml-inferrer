\section{Related Work}
\label{sec:related}

This section surveys related work in specification inference, test generation, and formal methods for software maintenance.

\subsection{Formal Specification Inference}

\subsubsection{Weakest Precondition Analysis}

The weakest precondition calculus~\cite{dijkstra_guarded_1975, dijkstra1976discipline} and Hoare logic~\cite{hoare1969axiomatic} form the theoretical foundation of our approach. While these techniques have been extensively studied in verification contexts, their application to automated specification inference for practical software engineering is more recent.

Barnett and Leino~\cite{barnett2005weakest} developed the Boogie intermediate language, which uses WP reasoning for verifying .NET programs. Our work differs by focusing on specification \emph{inference} rather than verification against provided specifications.

\subsubsection{Dynamic Invariant Detection}

Daikon~\cite{ernst2007daikon} pioneered dynamic invariant detection, inferring likely specifications from program execution traces. Unlike our static approach:
\begin{itemize}
    \item Daikon requires execution traces, which may not cover all paths
    \item Inferred invariants are ``likely'' rather than guaranteed
    \item Applicability depends on test suite quality
\end{itemize}

Perkins et al.~\cite{perkins2009automatically} extended dynamic inference to detect and repair program errors. Related work on specification mining~\cite{ammons2002mining, yang2006automatically, shoham2008static} has explored extracting temporal properties and API usage patterns from execution traces. Our static approach complements these dynamic methods by providing guaranteed-correct specifications that can seed dynamic refinement.

\subsubsection{Natural Language Processing Approaches}

Toradocu~\cite{toradocu2016} and its successor Jdoctor~\cite{jdoctor2018} use NLP to extract specifications from Javadoc comments. As discussed in Section~\ref{sec:results}, these tools are complementary to our approach:
\begin{itemize}
    \item Jdoctor achieves 92\% precision and 83\% recall on documented exceptions
    \item Our tool achieves 94.2\% precision and 89.3\% recall from code analysis
    \item 735 specifications were uniquely found by Jdoctor; 1,091 uniquely by JML-Inferrer
\end{itemize}

Pandita et al.~\cite{pandita2012inferring} used NLP to infer resource specifications from API documentation. Their approach is limited to documented behaviors, while our code-based analysis can infer undocumented constraints.

\subsubsection{Symbolic Execution Approaches}

Symbolic execution~\cite{king1976symbolic} has been widely applied to program analysis and test generation. KLEE~\cite{cadar2008klee} and DART~\cite{godefroid2005dart} pioneered practical symbolic execution for automated testing. PreInfer~\cite{preinfer2017} infers preconditions using symbolic execution via Microsoft's Pex framework. Key differences from our approach:
\begin{itemize}
    \item PreInfer uses dynamic symbolic execution; we use static analysis
    \item PreInfer handles complex conditions including quantifiers; we focus on scalable inference of simpler conditions
    \item PreInfer targets C\#; we target Java
\end{itemize}

\subsection{LLM-Based Specification and Test Generation}

\subsubsection{LLM Specification Inference}

Recent work has explored using LLMs for specification inference. Ma et al.~\cite{ma2024specgen} propose SpecGen, which uses GPT-4 to generate specifications from code. Endres et al.~\cite{endres2024can} investigate whether LLMs can transform natural language intent into formal postconditions, finding that while LLMs show promise, they struggle with complex specifications. A comprehensive survey by Jiang et al.~\cite{jiang2024survey} covers LLMs for code generation, including specification-related tasks. Compared to our approach:
\begin{itemize}
    \item LLMs achieve higher recall (91.2\% vs 89.3\%) but lower precision (78.6\% vs 94.2\%)
    \item LLM outputs are non-deterministic (67.3\% consistency across runs)
    \item Static analysis is reproducible and explainable
\end{itemize}

\subsubsection{LLM-Based Test Generation}

LLMs have shown promise for automated test generation~\cite{chen2022codet, schafer2023adaptive, chen2021codex}. The Codex model~\cite{chen2021codex} demonstrated that large language models trained on code can generate functionally correct programs, spurring research into LLM-based testing~\cite{deng2023large, yuan2024chatunitest}. Sch\"{a}fer et al.~\cite{schafer2023adaptive} use adaptive test generation with LLMs, achieving 70\% compilation rates. Our work demonstrates that providing formal specifications to LLMs improves both compilation rates (91.5\%) and test quality (mutation score).

Lemieux et al.~\cite{lemieux2023codamosa} combine coverage-guided fuzzing with LLM test generation. Their approach could potentially benefit from specification-guided generation, which we leave as future work.

\subsection{Test Oracle Generation}

\subsubsection{Specification-Based Oracles}

Specification-based testing~\cite{zhu_software_1997, ammann2016introduction} has long used formal specifications as test oracles. Traditional automated test generation tools such as EvoSuite~\cite{fraser2011evosuite}, Randoop~\cite{pacheco2007randoop}, and Korat~\cite{boyapati2002korat} focus on generating inputs but struggle with oracle generation. Our contribution is automating the specification inference step, reducing the manual effort required and enabling specification-guided test generation.

Peters and Parnas~\cite{peters1998using} demonstrated using formal documentation as test oracles. Our work extends this by generating specifications where documentation is absent.

\subsubsection{Metamorphic Testing}

Metamorphic testing~\cite{chen2018metamorphic} uses metamorphic relations as implicit oracles. While not directly comparable, inferred specifications can be viewed as explicit oracles that enable traditional assertion-based testing.

\subsection{Formal Methods in Practice}

\subsubsection{Industrial Formal Verification}

Large-scale formal verification projects~\cite{andronick_large-scale_2012, klein2014sel4} demonstrate both the value and cost of formal methods. Woodcock et al.~\cite{woodcock2009formal} surveyed industrial applications of formal methods, finding that specification effort remains a key barrier. Matichuk et al.~\cite{matichuk_cost_2015} report that specification development accounts for approximately 50\% of formal verification effort. Our automation aims to reduce this barrier.

\subsubsection{Lightweight Formal Methods}

Zimmermann and Wehrheim~\cite{zimmermann2019lightweight} advocate for lightweight specifications compatible with DevOps practices. Our automatically inferred specifications align with this philosophy: they are machine-checkable but generated without manual annotation effort.

\subsubsection{Contract-Based Design}

Design by Contract~\cite{meyer1992design} and the JML language~\cite{jml, leavens2006design} promote specification as a design activity. Extended Static Checking for Java (ESC/Java)~\cite{flanagan2002extended, chalin2010jml} pioneered automatic verification of JML specifications, and modern tools like Facebook Infer~\cite{calcagno2015infer} and FindBugs~\cite{hovemeyer2004finding} apply static analysis at scale. Our tool supports this paradigm by providing initial specifications that developers can refine.

\subsubsection{Formal Verification Tools}

Several tools verify Java code against JML specifications:

\begin{itemize}
    \item \textbf{OpenJML}: Provides runtime assertion checking and static verification via SMT solvers. Unlike our inference approach, OpenJML \emph{verifies} user-provided specifications rather than generating them.

    \item \textbf{KeY}: A deductive verification system for Java that proves program correctness using interactive theorem proving. KeY requires complete specifications and significant user guidance.

    \item \textbf{JMLUnitNG}: Generates unit tests from JML specifications, complementing our approach---we generate specifications that tools like JMLUnitNG can consume.
\end{itemize}

Our tool is complementary to these verification tools: we generate specifications automatically, which can then be verified or used for test generation by existing tools.

\subsection{Program Comprehension}

\subsubsection{Method Stereotypes}

Dragan et al.~\cite{dragan2006reverse} introduced method stereotypes for reverse engineering. Our categorization taxonomy extends their work with categories relevant to specification inference. Table~\ref{tab:stereotype-comparison} maps our categories to their stereotypes.

\begin{table}[h]
\centering
\caption{Mapping to Dragan et al. stereotypes}
\label{tab:stereotype-comparison}
\begin{tabular}{ll}
\toprule
\textbf{Our Category} & \textbf{Dragan Stereotype} \\
\midrule
Accessors \& Mutators & Get, Set \\
Factory \& Delegate & Factory, Collaborator \\
Control Flow & Command, Controller \\
Utility & Property, Predicate \\
State Modification & Command, Non-void-Command \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Code Summarization}

LLM-based code summarization~\cite{ahmed2022few} can complement specifications by providing natural language descriptions. Specifications provide formal, machine-checkable contracts while summaries provide human-readable overviews.

\subsection{Mutation Testing}

PiTest~\cite{coles2016pitest} is the standard mutation testing tool for Java, which we use for evaluation. Prior work~\cite{papadakis2019mutation} surveys mutation testing practices; our results contribute evidence that specification-guided tests achieve higher mutation scores than ad-hoc generation.

\subsection{Summary of Positioning}

Our work occupies a unique position in the landscape:

\begin{enumerate}
    \item \textbf{Compared to dynamic analysis} (Daikon): We provide guaranteed-correct specifications without requiring execution traces.

    \item \textbf{Compared to NLP approaches} (Jdoctor): We infer specifications from code, not documentation, achieving broader coverage.

    \item \textbf{Compared to LLM approaches}: We achieve higher precision and determinism through formal analysis.

    \item \textbf{Compared to verification tools} (Boogie): We focus on inference rather than verification, reducing manual effort.
\end{enumerate}

The key novelty is demonstrating that WP/SP-based static analysis, combined with practical heuristics for loop handling, can scale to large codebases while maintaining high precision.
